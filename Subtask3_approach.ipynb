{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate\n",
        "!pip install rouge_score\n",
        "!pip install bert_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzwYwROG3r_m",
        "outputId": "520f87c2-4112-4774-989d-1e7355a36e94"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.3)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.2.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.26.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.10)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.6)\n",
            "Requirement already satisfied: bert_score in /usr/local/lib/python3.10/dist-packages (0.3.13)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.5.1+cu121)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.2.2)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.46.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bert_score) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.66.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert_score) (3.8.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert_score) (24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2024.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (2024.9.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.26.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.4.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (3.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2024.8.30)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1oTHUsky2Yvx"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "# from evaluate import load\n",
        "# import os\n",
        "\n",
        "# # Load evaluation metrics\n",
        "# bertscore = load(\"bertscore\")\n",
        "# rouge = load(\"rouge\")\n",
        "\n",
        "# # Load annotations\n",
        "# def load_annotations(file_path):\n",
        "#     data = []\n",
        "#     with open(file_path, 'r', encoding='utf-8') as f:\n",
        "#         for line in f:\n",
        "#             fields = line.strip().split('\\t')\n",
        "#             if len(fields) >= 4:\n",
        "#                 article_id, narrative, subnarrative, explanation = fields\n",
        "#                 data.append([article_id, narrative, subnarrative, explanation])\n",
        "#     return pd.DataFrame(data, columns=[\"article_id\", \"narrative\", \"subnarrative\", \"explanation\"])\n",
        "\n",
        "# # Generate grounded explanations\n",
        "# def generate_grounded_explanations(articles_dir, annotations_df, tokenizer, model):\n",
        "#     explanations = []\n",
        "#     for _, row in annotations_df.iterrows():\n",
        "#         article_path = os.path.join(articles_dir, row['article_id'])\n",
        "#         if os.path.exists(article_path):\n",
        "#             with open(article_path, 'r', encoding='utf-8') as f:\n",
        "#                 article_text = f.read()\n",
        "\n",
        "#             prompt = f\"Summarize the key points supporting the claim: {row['narrative']} - {row['subnarrative']}. Ground your explanation in the text.\"\n",
        "#             inputs = tokenizer(prompt + article_text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "#             outputs = model.generate(inputs[\"input_ids\"], max_length=80, min_length=20, num_beams=4, early_stopping=True)\n",
        "#             explanation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "#             explanations.append(explanation)\n",
        "#         else:\n",
        "#             explanations.append(\"\")\n",
        "#     return explanations\n",
        "\n",
        "# # Evaluate explanations\n",
        "# def evaluate_explanations(annotations_df, generated_explanations):\n",
        "#     references = annotations_df['explanation'].tolist()\n",
        "#     results = {}\n",
        "\n",
        "#     # ROUGE Scores\n",
        "#     rouge_scores = rouge.compute(predictions=generated_explanations, references=references)\n",
        "#     results['rouge1'] = rouge_scores['rouge1']\n",
        "#     results['rouge2'] = rouge_scores['rouge2']\n",
        "#     results['rougeL'] = rouge_scores['rougeL']\n",
        "\n",
        "#     # BERTScore\n",
        "#     bert_scores = bertscore.compute(predictions=generated_explanations, references=references, model_type=\"distilbert-base-uncased\")\n",
        "#     results['bert_precision'] = sum(bert_scores['precision']) / len(bert_scores['precision'])\n",
        "#     results['bert_recall'] = sum(bert_scores['recall']) / len(bert_scores['recall'])\n",
        "#     results['bert_f1'] = sum(bert_scores['f1']) / len(bert_scores['f1'])\n",
        "\n",
        "#     return results\n",
        "\n",
        "# # Main script\n",
        "# annotations_file = 'EN/subtask-3-annotations.txt'\n",
        "# articles_dir = 'EN/raw-documents'\n",
        "\n",
        "# # Load tokenizer and model\n",
        "# model_name = \"facebook/bart-large-cnn\"\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# # Load annotations\n",
        "# annotations_df = load_annotations(annotations_file)\n",
        "\n",
        "# # Generate grounded explanations\n",
        "# annotations_df['generated_explanation'] = generate_grounded_explanations(articles_dir, annotations_df, tokenizer, model)\n",
        "\n",
        "# # Evaluate generated explanations\n",
        "# evaluation_results = evaluate_explanations(annotations_df, annotations_df['generated_explanation'].tolist())\n",
        "\n",
        "# # Print evaluation results\n",
        "# print(f\"Evaluation Results: {evaluation_results}\")\n",
        "\n",
        "# # # Save results\n",
        "# # annotations_df.to_csv('/mnt/data/subtask3_results.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from evaluate import load\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# Load evaluation metrics\n",
        "bertscore = load(\"bertscore\")\n",
        "rouge = load(\"rouge\")\n",
        "\n",
        "# Load annotations\n",
        "def load_annotations(file_path):\n",
        "    data = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            fields = line.strip().split('\\t')\n",
        "            if len(fields) >= 4:\n",
        "                article_id, narrative, subnarrative, explanation = fields\n",
        "                data.append([article_id, narrative, subnarrative, explanation])\n",
        "    return pd.DataFrame(data, columns=[\"article_id\", \"narrative\", \"subnarrative\", \"explanation\"])\n",
        "\n",
        "# Generate grounded explanations with GPU and batch processing\n",
        "def generate_grounded_explanations(articles_dir, annotations_df, tokenizer, model, batch_size=8, max_length=80):\n",
        "    \"\"\"\n",
        "    Generate explanations in batches using GPU for efficiency.\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    explanations = []\n",
        "\n",
        "    for i in tqdm(range(0, len(annotations_df), batch_size), desc=\"Generating Explanations\"):\n",
        "        batch_data = annotations_df.iloc[i:i + batch_size]\n",
        "        batch_prompts = []\n",
        "        for _, row in batch_data.iterrows():\n",
        "            article_path = os.path.join(articles_dir, row['article_id'])\n",
        "            if os.path.exists(article_path):\n",
        "                with open(article_path, 'r', encoding='utf-8') as f:\n",
        "                    article_text = f.read()\n",
        "                prompt = (\n",
        "                    f\"Summarize the key points supporting the claim: {row['narrative']} - {row['subnarrative']}. \"\n",
        "                    \"Ground your explanation in the text. \"\n",
        "                    f\"{article_text}\"\n",
        "                )\n",
        "                batch_prompts.append(prompt)\n",
        "            else:\n",
        "                batch_prompts.append(\"Missing article content.\")\n",
        "\n",
        "        # Tokenize batch\n",
        "        inputs = tokenizer(\n",
        "            batch_prompts,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512\n",
        "        ).to(device)\n",
        "\n",
        "        # Generate explanations\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                inputs[\"input_ids\"],\n",
        "                attention_mask=inputs[\"attention_mask\"],\n",
        "                max_length=max_length,\n",
        "                num_beams=2,\n",
        "                early_stopping=True\n",
        "            )\n",
        "\n",
        "        # Decode generated text\n",
        "        explanations.extend([tokenizer.decode(output, skip_special_tokens=True) for output in outputs])\n",
        "\n",
        "    return explanations\n",
        "\n",
        "# Evaluate explanations\n",
        "def evaluate_explanations(annotations_df, generated_explanations):\n",
        "    references = annotations_df['explanation'].tolist()\n",
        "    results = {}\n",
        "\n",
        "    # ROUGE Scores\n",
        "    rouge_scores = rouge.compute(predictions=generated_explanations, references=references)\n",
        "    results['rouge1'] = rouge_scores['rouge1']\n",
        "    results['rouge2'] = rouge_scores['rouge2']\n",
        "    results['rougeL'] = rouge_scores['rougeL']\n",
        "\n",
        "    # BERTScore\n",
        "    bert_scores = bertscore.compute(predictions=generated_explanations, references=references, model_type=\"distilbert-base-uncased\")\n",
        "    results['bert_precision'] = sum(bert_scores['precision']) / len(bert_scores['precision'])\n",
        "    results['bert_recall'] = sum(bert_scores['recall']) / len(bert_scores['recall'])\n",
        "    results['bert_f1'] = sum(bert_scores['f1']) / len(bert_scores['f1'])\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "annotations_file = 'BG/subtask-3-annotations.txt'\n",
        "articles_dir = 'BG/raw-documents'\n",
        "\n",
        "# Load tokenizer and model\n",
        "# model_name = \"facebook/bart-large-cnn\"\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# from transformers import MBartForConditionalGeneration, MBartTokenizer\n",
        "\n",
        "# model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50\")\n",
        "# tokenizer = MBartTokenizer.from_pretrained(\"facebook/mbart-large-50\")\n",
        "\n",
        "model_name = \"google/mt5-large\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# model_name = \"bigscience/bloomz-7b1\"\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "\n",
        "# Load annotations\n",
        "annotations_df = load_annotations(annotations_file)\n",
        "\n",
        "# Generate grounded explanations\n",
        "print(\"Generating explanations...\")\n",
        "annotations_df['generated_explanation'] = generate_grounded_explanations(\n",
        "    articles_dir,\n",
        "    annotations_df,\n",
        "    tokenizer,\n",
        "    model,\n",
        "    batch_size=8,  # Adjust batch size based on GPU memory\n",
        "    max_length=80\n",
        ")\n",
        "\n",
        "# Evaluate generated explanations\n",
        "print(\"Evaluating explanations...\")\n",
        "evaluation_results = evaluate_explanations(annotations_df, annotations_df['generated_explanation'].tolist())\n",
        "\n",
        "# Print evaluation results\n",
        "print(f\"Evaluation Results: {evaluation_results}\")\n",
        "\n",
        "# # Save results\n",
        "# results_path = '/mnt/data/subtask3_results.csv'\n",
        "# annotations_df.to_csv(results_path, index=False)\n",
        "# print(f\"Results saved to {results_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uu9YfsXJ3U2j",
        "outputId": "d1ba9ecd-4d7e-4369-8db5-a666071a1c28"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating explanations...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Explanations: 100%|██████████| 45/45 [04:47<00:00,  6.40s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating explanations...\n",
            "Evaluation Results: {'rouge1': 0.0017191689915329361, 'rouge2': 0.0, 'rougeL': 0.0015153518318960003, 'bert_precision': 0.5744952647959819, 'bert_recall': 0.639760310993809, 'bert_f1': 0.6029345385667657}\n"
          ]
        }
      ]
    }
  ]
}