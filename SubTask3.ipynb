{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_annotations(file_path):\n",
    "    data = []\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            fields = line.strip().split('\\t')\n",
    "            if len(fields) >= 4:\n",
    "                article_id = fields[0]\n",
    "                narrative_field = fields[1]\n",
    "                subnarrative_field = fields[2]\n",
    "                explanation = fields[3]\n",
    "\n",
    "                # Split narratives and sub-narratives if they contain semicolons\n",
    "                narratives = narrative_field.split(';')\n",
    "                subnarratives = subnarrative_field.split(';')\n",
    "\n",
    "                # Append processed data to the list\n",
    "                data.append([article_id, narratives, subnarratives, explanation])\n",
    "    \n",
    "    # Convert the data list into a pandas DataFrame\n",
    "    return pd.DataFrame(data, columns=[\"article_id\", \"narratives\", \"subnarratives\", \"explanation\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_articles(raw_documents_folder):\n",
    "    articles = {}\n",
    "    for filename in os.listdir(raw_documents_folder):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            article_id = filename.split('.')[0]\n",
    "            with open(os.path.join(raw_documents_folder, filename), 'r', encoding='utf-8') as f:\n",
    "                articles[article_id] = f.read()\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = load_annotations('./EN/subtask-3-annotations.txt')\n",
    "articles = load_all_articles('./EN/raw-documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_articles_with_annotations(annotations, articles):\n",
    "    # Convert articles dictionary to a DataFrame\n",
    "    articles_df = pd.DataFrame(list(articles.items()), columns=[\"article_id\", \"article_text\"])\n",
    "    # Normalize article_id in both DataFrames\n",
    "    annotations[\"article_id\"] = annotations[\"article_id\"].str.strip().str.lower().str.replace(\".txt\", \"\")\n",
    "    articles_df[\"article_id\"] = articles_df[\"article_id\"].str.strip().str.lower()\n",
    "    \n",
    "    # Perform a join\n",
    "    merged_df = annotations.set_index(\"article_id\").join(articles_df.set_index(\"article_id\"), on=\"article_id\")\n",
    "    \n",
    "    # Reset the index if needed\n",
    "    merged_df.reset_index(inplace=True)\n",
    "    \n",
    "    return merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_mapped  = join_articles_with_annotations(annotations, articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                                                  input  \\\n",
      "0    narrative: CC: Criticism of climate movement s...   \n",
      "1    narrative: CC: Questioning the measurements an...   \n",
      "2    narrative: CC: Criticism of climate movement s...   \n",
      "3    narrative: URW: Speculating war outcomes subna...   \n",
      "4    narrative: URW: Praise of Russia subnarrative:...   \n",
      "..                                                 ...   \n",
      "198  narrative: CC: Criticism of climate movement s...   \n",
      "199  narrative: URW: Discrediting Ukraine subnarrat...   \n",
      "200  narrative: URW: Negative Consequences for the ...   \n",
      "201  narrative: CC: Questioning the measurements an...   \n",
      "202  narrative: URW: Blaming the war on others rath...   \n",
      "\n",
      "                                                output  \n",
      "0    The text accuses climate activist Bill Gates f...  \n",
      "1    There are inconsistencies in the predictions o...  \n",
      "2    The article talks about climate activists atta...  \n",
      "3    The text conveys a narrative depicting negativ...  \n",
      "4    Multiple paragraphs within the text present al...  \n",
      "..                                                 ...  \n",
      "198  The article continuously criticizes climate mo...  \n",
      "199  The article covers several topics, talking abo...  \n",
      "200  The text presents multiple paragraphs in which...  \n",
      "201  The author argues that CO2 has no effect on th...  \n",
      "202  The text presents several passages that togeth...  \n",
      "\n",
      "[203 rows x 2 columns]>\n"
     ]
    }
   ],
   "source": [
    "def prepare_data_for_model(annotations_mapped):\n",
    "    annotations_mapped[\"input\"] = (\n",
    "        \"narrative: \" + annotations[\"narratives\"].apply(lambda x: \"; \".join(x)) +\n",
    "        \" subnarrative: \" + annotations[\"subnarratives\"].apply(lambda x: \"; \".join(x)) +\n",
    "        \" context: \" + annotations_mapped[\"article_text\"]\n",
    "    )\n",
    "    annotations_mapped[\"output\"] = annotations_mapped[\"explanation\"]\n",
    "    return annotations_mapped[[\"input\", \"output\"]]\n",
    "\n",
    "training_data = prepare_data_for_model(annotations_mapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t5_summarize(article, max_length=150, min_length=40):\n",
    "    inputs = tokenizer.encode(\"summarize: \" + article, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    summary_ids = model.generate(inputs, max_length=max_length, min_length=min_length, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# # Load the dataset into a pandas DataFrame\n",
    "# df = pd.read_csv(dataset_path)  # Adjust to read_excel or read_json as needed\n",
    "# # Summarize each article in the dataset\n",
    "# # Ensure the column name 'article' matches your dataset's column for text data\n",
    "# df['generated'] = df['article'].apply(lambda x: t5_summarize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hasan\\AppData\\Local\\Temp\\ipykernel_39580\\2593263378.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data['generated'] = training_data['input'].apply(lambda x: t5_summarize(x))\n"
     ]
    }
   ],
   "source": [
    "training_data['generated'] = training_data['input'].apply(lambda x: t5_summarize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in c:\\users\\hasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.4.3)\n",
      "Requirement already satisfied: datasets>=2.0.0 in c:\\users\\hasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from evaluate) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from evaluate) (2.0.2)\n",
      "Requirement already satisfied: dill in c:\\users\\hasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\hasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\hasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\hasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from evaluate) (4.67.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\hasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\hasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in c:\\users\\hasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\hasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from evaluate) (0.26.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\hasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from evaluate) (24.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\hasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\hasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets>=2.0.0->evaluate) (18.0.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\hasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.11.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.19.0->evaluate) (2024.7.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\hasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\hasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\hasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\hasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\hasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\hasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\hasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\hasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.17.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hasan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate\n",
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average bertScore: 0.7581\n"
     ]
    }
   ],
   "source": [
    "bertscore = load(\"bertscore\")\n",
    "# Function to calculate bertScore\n",
    "def calculate_bert_scores(references, generations):\n",
    "    bert_scores = bertscore.compute(predictions = generations, references=references, model_type=\"distilbert-base-uncased\")\n",
    "    return bert_scores['f1']\n",
    "\n",
    "# Prepare data\n",
    "references = training_data['output']\n",
    "generations = training_data['generated']\n",
    "\n",
    "# Calculate bert scores\n",
    "bert_scores = calculate_bert_scores(references, generations)\n",
    "\n",
    "# Add scores to the DataFrame\n",
    "training_data['bert_score'] = bert_scores\n",
    "# print(training_data)\n",
    "# Calculate and print average score\n",
    "avg_bert_score = sum(bert_scores) / len(bert_scores) if bert_scores else 0\n",
    "print(f\"Average bertScore: {avg_bert_score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
