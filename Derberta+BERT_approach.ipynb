{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade tensorflow\n",
        "!pip install iterstrat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUc8B1kuP1Ts",
        "outputId": "4129de9c-0bd0-47c1-cf37-a533c5c8d663"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.1)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement iterstrat (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for iterstrat\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing Annotations"
      ],
      "metadata": {
        "id": "FfP46ALcKYRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ENTITY_TAXONOMY = {\n",
        "    \"Antagonist\": [\"Bigot\", \"Conspirator\", \"Corrupt\", \"Deceiver\", \"Terrorist\", \"Traitor\", \"Tyrant\"],\n",
        "    \"Protagonist\": [\"Guardian\", \"Martyr\", \"Peacemaker\", \"Rebel\", \"Virtuous\"],\n",
        "    \"Innocent\": [\"Exploited\", \"Victim\", \"Underdog\"]\n",
        "}"
      ],
      "metadata": {
        "id": "KruZiBdwP1Hj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "x0dTJ8r7KSfK",
        "outputId": "b2353bbc-a26e-4692-f32e-1aa260058899"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           filename           entity  start_idx  end_idx    main_role  \\\n",
              "0  EN_CC_100013.txt       Bill Gates         93      102   Antagonist   \n",
              "1  EN_CC_100013.txt              BBC       1860     1862   Antagonist   \n",
              "2  EN_CC_100013.txt  Jeffrey Epstein       2005     2019   Antagonist   \n",
              "3  EN_UA_300009.txt     Fail Alsynov        176      187  Protagonist   \n",
              "4  EN_UA_300009.txt   Bashkir people       1616     1629     Innocent   \n",
              "\n",
              "            fine_roles  \n",
              "0  [Deceiver, Corrupt]  \n",
              "1           [Deceiver]  \n",
              "2            [Corrupt]  \n",
              "3      [Rebel, Martyr]  \n",
              "4             [Victim]  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-21137468-e5e7-4e40-8014-23e638c93577\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>entity</th>\n",
              "      <th>start_idx</th>\n",
              "      <th>end_idx</th>\n",
              "      <th>main_role</th>\n",
              "      <th>fine_roles</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>EN_CC_100013.txt</td>\n",
              "      <td>Bill Gates</td>\n",
              "      <td>93</td>\n",
              "      <td>102</td>\n",
              "      <td>Antagonist</td>\n",
              "      <td>[Deceiver, Corrupt]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>EN_CC_100013.txt</td>\n",
              "      <td>BBC</td>\n",
              "      <td>1860</td>\n",
              "      <td>1862</td>\n",
              "      <td>Antagonist</td>\n",
              "      <td>[Deceiver]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>EN_CC_100013.txt</td>\n",
              "      <td>Jeffrey Epstein</td>\n",
              "      <td>2005</td>\n",
              "      <td>2019</td>\n",
              "      <td>Antagonist</td>\n",
              "      <td>[Corrupt]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>EN_UA_300009.txt</td>\n",
              "      <td>Fail Alsynov</td>\n",
              "      <td>176</td>\n",
              "      <td>187</td>\n",
              "      <td>Protagonist</td>\n",
              "      <td>[Rebel, Martyr]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>EN_UA_300009.txt</td>\n",
              "      <td>Bashkir people</td>\n",
              "      <td>1616</td>\n",
              "      <td>1629</td>\n",
              "      <td>Innocent</td>\n",
              "      <td>[Victim]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-21137468-e5e7-4e40-8014-23e638c93577')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-21137468-e5e7-4e40-8014-23e638c93577 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-21137468-e5e7-4e40-8014-23e638c93577');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a94ae5e2-3e7c-4cf0-bad0-2dbe1bc0a826\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a94ae5e2-3e7c-4cf0-bad0-2dbe1bc0a826')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a94ae5e2-3e7c-4cf0-bad0-2dbe1bc0a826 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "annotations",
              "summary": "{\n  \"name\": \"annotations\",\n  \"rows\": 686,\n  \"fields\": [\n    {\n      \"column\": \"filename\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 202,\n        \"samples\": [\n          \"EN_UA_012803.txt\",\n          \"EN_UA_025652.txt\",\n          \"EN_UA_025764.txt\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"entity\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 375,\n        \"samples\": [\n          \"United Nations\",\n          \"Brexit Britain\",\n          \"London\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"start_idx\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1037,\n        \"min\": 37,\n        \"max\": 5515,\n        \"num_unique_values\": 600,\n        \"samples\": [\n          2519,\n          981,\n          842\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"end_idx\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1037,\n        \"min\": 44,\n        \"max\": 5521,\n        \"num_unique_values\": 598,\n        \"samples\": [\n          2528,\n          1450,\n          3524\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"main_role\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Antagonist\",\n          \"Protagonist\",\n          \"Innocent\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fine_roles\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "# Path to the annotations file\n",
        "annotations_path = \"EN/subtask-1-annotations.txt\"\n",
        "\n",
        "# Read the annotations file manually and ensure all columns are handled\n",
        "with open(annotations_path, \"r\") as f:\n",
        "    data = []\n",
        "    for line in f:\n",
        "        parts = line.strip().split(\"\\t\")\n",
        "        filename, entity, start_idx, end_idx, main_role = parts[:5]\n",
        "        fine_roles = parts[5:]  # Remaining parts are fine-grained roles\n",
        "        data.append([filename, entity, int(start_idx), int(end_idx), main_role, fine_roles])\n",
        "\n",
        "# Convert to DataFrame\n",
        "columns = [\"filename\", \"entity\", \"start_idx\", \"end_idx\", \"main_role\", \"fine_roles\"]\n",
        "annotations = pd.DataFrame(data, columns=columns)\n",
        "\n",
        "# Setup device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "# Display the first few rows\n",
        "annotations.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Raw Documents"
      ],
      "metadata": {
        "id": "Gz_wIq16NLXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Path to the raw documents folder\n",
        "raw_documents_path = \"EN/raw-documents\"\n",
        "\n",
        "# Load all documents into a dictionary\n",
        "documents = {}\n",
        "for filename in os.listdir(raw_documents_path):\n",
        "    filepath = os.path.join(raw_documents_path, filename)\n",
        "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "        documents[filename] = f.read()\n",
        "\n",
        "# Map annotations to document content\n",
        "def extract_context(row, window=225):\n",
        "    \"\"\"Extracts the context around an entity from the document.\"\"\"\n",
        "    doc_text = documents[row[\"filename\"]]\n",
        "    start_idx, end_idx = row[\"start_idx\"], row[\"end_idx\"]\n",
        "\n",
        "    # Define context window\n",
        "    start_context = max(0, start_idx - window)\n",
        "    end_context = min(len(doc_text), end_idx + window)\n",
        "\n",
        "    # Highlight the entity in the context\n",
        "    context = doc_text[start_context:start_idx] + \"[ENTITY]\" + doc_text[start_idx:end_idx] + \"[/ENTITY]\" + doc_text[end_idx:end_context]\n",
        "    return context\n",
        "\n",
        "# Apply the context extraction\n",
        "annotations[\"context\"] = annotations.apply(extract_context, axis=1)\n",
        "\n",
        "# Display a sample row with context\n",
        "annotations.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "zK9bxWa2NMe8",
        "outputId": "cd00b13e-b720-4389-8590-71ab854c2939"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           filename           entity  start_idx  end_idx    main_role  \\\n",
              "0  EN_CC_100013.txt       Bill Gates         93      102   Antagonist   \n",
              "1  EN_CC_100013.txt              BBC       1860     1862   Antagonist   \n",
              "2  EN_CC_100013.txt  Jeffrey Epstein       2005     2019   Antagonist   \n",
              "3  EN_UA_300009.txt     Fail Alsynov        176      187  Protagonist   \n",
              "4  EN_UA_300009.txt   Bashkir people       1616     1629     Innocent   \n",
              "\n",
              "            fine_roles                                            context  \n",
              "0  [Deceiver, Corrupt]  Bill Gates Says He Is ‘The Solution’ To Climat...  \n",
              "1           [Deceiver]  s, according to data from the World Bank.\\n\\nE...  \n",
              "2            [Corrupt]   by ‘conspiracy theorists’ for pushing vaccine...  \n",
              "3      [Rebel, Martyr]  Russia: Clashes erupt in Bashkortostan as righ...  \n",
              "4             [Victim]  e. \\n\\nAlsynov contends the Bashkir words mean...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2c0fecb8-03cc-40ee-90ea-f734cdf4ae27\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>entity</th>\n",
              "      <th>start_idx</th>\n",
              "      <th>end_idx</th>\n",
              "      <th>main_role</th>\n",
              "      <th>fine_roles</th>\n",
              "      <th>context</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>EN_CC_100013.txt</td>\n",
              "      <td>Bill Gates</td>\n",
              "      <td>93</td>\n",
              "      <td>102</td>\n",
              "      <td>Antagonist</td>\n",
              "      <td>[Deceiver, Corrupt]</td>\n",
              "      <td>Bill Gates Says He Is ‘The Solution’ To Climat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>EN_CC_100013.txt</td>\n",
              "      <td>BBC</td>\n",
              "      <td>1860</td>\n",
              "      <td>1862</td>\n",
              "      <td>Antagonist</td>\n",
              "      <td>[Deceiver]</td>\n",
              "      <td>s, according to data from the World Bank.\\n\\nE...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>EN_CC_100013.txt</td>\n",
              "      <td>Jeffrey Epstein</td>\n",
              "      <td>2005</td>\n",
              "      <td>2019</td>\n",
              "      <td>Antagonist</td>\n",
              "      <td>[Corrupt]</td>\n",
              "      <td>by ‘conspiracy theorists’ for pushing vaccine...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>EN_UA_300009.txt</td>\n",
              "      <td>Fail Alsynov</td>\n",
              "      <td>176</td>\n",
              "      <td>187</td>\n",
              "      <td>Protagonist</td>\n",
              "      <td>[Rebel, Martyr]</td>\n",
              "      <td>Russia: Clashes erupt in Bashkortostan as righ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>EN_UA_300009.txt</td>\n",
              "      <td>Bashkir people</td>\n",
              "      <td>1616</td>\n",
              "      <td>1629</td>\n",
              "      <td>Innocent</td>\n",
              "      <td>[Victim]</td>\n",
              "      <td>e. \\n\\nAlsynov contends the Bashkir words mean...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2c0fecb8-03cc-40ee-90ea-f734cdf4ae27')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2c0fecb8-03cc-40ee-90ea-f734cdf4ae27 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2c0fecb8-03cc-40ee-90ea-f734cdf4ae27');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2b931bc0-6212-4d0a-8a76-36f48559137d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2b931bc0-6212-4d0a-8a76-36f48559137d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2b931bc0-6212-4d0a-8a76-36f48559137d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "annotations",
              "summary": "{\n  \"name\": \"annotations\",\n  \"rows\": 686,\n  \"fields\": [\n    {\n      \"column\": \"filename\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 202,\n        \"samples\": [\n          \"EN_UA_012803.txt\",\n          \"EN_UA_025652.txt\",\n          \"EN_UA_025764.txt\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"entity\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 375,\n        \"samples\": [\n          \"United Nations\",\n          \"Brexit Britain\",\n          \"London\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"start_idx\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1037,\n        \"min\": 37,\n        \"max\": 5515,\n        \"num_unique_values\": 600,\n        \"samples\": [\n          2519,\n          981,\n          842\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"end_idx\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1037,\n        \"min\": 44,\n        \"max\": 5521,\n        \"num_unique_values\": 598,\n        \"samples\": [\n          2528,\n          1450,\n          3524\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"main_role\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Antagonist\",\n          \"Protagonist\",\n          \"Innocent\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fine_roles\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 686,\n        \"samples\": [\n          \"umpur for Beijing, with 227 passengers and 12 crew members on board.\\n\\nIt is worth recalling that immediately after the MH17 plane crash on July 17 2014, prior to the conduct of a preliminary investigation, Secretary of State [ENTITY]John Kerr[/ENTITY]y and US Ambassador to the UN Samantha Power pointed their finger at Moscow without a shred of evidence. In turn, the allegations directed against Russia were used to justify the imposition of sweeping economic sanctions agai\",\n          \"even thinking about how blasphemous it sounds and looks. <\\u2026>\\n\\n\\\"Ukraine, in fact, has become a NATO PMC. It gets money, weapons and intelligence data. Ukraine receives instructions which targets to attack and where. It is the [ENTITY]Ukrainian peopl[/ENTITY]e who suffer from this in the first place and the people of Ukraine are forced to fight for the sake of someone else's tasks.\\\"\\n\\nHowever, the Russian Ambassador to the UN said, everything could have ended differently for Ukrai\",\n          \"tional level. Despite Senate Majority Leader Chuck Schumer\\u2019s reassurances that there is nothing to be concerned about, just last week his home state of New York prohibited gas stoves.\\n\\n\\u201cNobody is taking away your gas stove,\\u201d [ENTITY]Schume[/ENTITY]r tweeted in early February. \\u201cShameless and desperate MAGA Republicans are showing us they will cook up any distraction to divert from real issues the American people want solved, like the debt ceiling.\\u201d\\n\\nThat turned out to b\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare Training and Testing Datasets"
      ],
      "metadata": {
        "id": "5C0T84ajObGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Stratified split into training and testing sets\n",
        "train_data, test_data = train_test_split(\n",
        "    annotations,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=annotations[\"main_role\"]\n",
        ")\n",
        "\n",
        "# Define the main role classification model\n",
        "main_role_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"microsoft/deberta-base\", num_labels=3  # Three main roles\n",
        ")\n",
        "\n",
        "main_role_model.to(device)\n",
        "# Encode main roles\n",
        "main_role_encoder = LabelEncoder()\n",
        "train_data[\"main_role_encoded\"] = main_role_encoder.fit_transform(train_data[\"main_role\"])\n",
        "test_data[\"main_role_encoded\"] = main_role_encoder.transform(test_data[\"main_role\"])\n",
        "\n",
        "# Encode fine-grained roles\n",
        "fine_role_binarizer = MultiLabelBinarizer()\n",
        "train_data[\"fine_roles_encoded\"] = fine_role_binarizer.fit_transform(train_data[\"fine_roles\"]).tolist()\n",
        "test_data[\"fine_roles_encoded\"] = fine_role_binarizer.transform(test_data[\"fine_roles\"]).tolist()\n",
        "\n",
        "# Initialize the BERT tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
        "\n",
        "def tokenize_context(data, tokenizer):\n",
        "    \"\"\"Tokenize the context column.\"\"\"\n",
        "    return tokenizer(\n",
        "        data[\"context\"].tolist(),\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "# Tokenize context using BERT embeddings\n",
        "train_inputs = tokenize_context(train_data, tokenizer)\n",
        "test_inputs = tokenize_context(test_data, tokenizer)\n",
        "\n",
        "# # Generate TF-IDF embeddings\n",
        "# def compute_tfidf_embeddings(data):\n",
        "#     vectorizer = TfidfVectorizer(\n",
        "#         max_features=5000,  # Limit to top 5000 features\n",
        "#         ngram_range=(1, 5),  # Include unigrams and bigrams\n",
        "#         stop_words=\"english\"  # Remove common stopwords\n",
        "#     )\n",
        "#     tfidf_matrix = vectorizer.fit_transform(data[\"context\"])\n",
        "#     return tfidf_matrix.toarray(), vectorizer\n",
        "\n",
        "# # Compute TF-IDF embeddings for train and test data\n",
        "# tfidf_embeddings_train, tfidf_vectorizer = compute_tfidf_embeddings(train_data)\n",
        "# tfidf_embeddings_test = tfidf_vectorizer.transform(test_data[\"context\"]).toarray()\n",
        "\n",
        "# # Combine TF-IDF and BERT embeddings\n",
        "# def combine_embeddings(bert_embeddings, tfidf_embeddings):\n",
        "#     \"\"\"Concatenate BERT and TF-IDF embeddings.\"\"\"\n",
        "#     return np.hstack((bert_embeddings, tfidf_embeddings))\n",
        "\n",
        "# # Extract BERT embeddings\n",
        "# def extract_bert_embeddings(inputs, model):\n",
        "#     \"\"\"Extract embeddings (CLS token) from the model.\"\"\"\n",
        "#     with torch.no_grad():\n",
        "#         # Identify the base model attribute dynamically\n",
        "#         base_model = getattr(model, model.base_model_prefix)\n",
        "#         outputs = base_model(inputs[\"input_ids\"].to(device), attention_mask=inputs[\"attention_mask\"].to(device))\n",
        "#     return outputs.last_hidden_state[:, 0, :].cpu().numpy()  # CLS token\n",
        "\n",
        "# # Assuming the DeBERTa model is already loaded as `main_role_model`\n",
        "# train_bert_embeddings = extract_bert_embeddings(train_inputs, main_role_model)\n",
        "# test_bert_embeddings = extract_bert_embeddings(test_inputs, main_role_model)\n",
        "\n",
        "# # Combine BERT and TF-IDF embeddings\n",
        "# train_combined_embeddings = combine_embeddings(train_bert_embeddings, tfidf_embeddings_train)\n",
        "# test_combined_embeddings = combine_embeddings(test_bert_embeddings, tfidf_embeddings_test)\n",
        "\n",
        "# Display information about the processed data\n",
        "print(\"Main Role Encoder Classes:\", main_role_encoder.classes_)\n",
        "print(\"Fine Role Classes:\", fine_role_binarizer.classes_)\n",
        "# print(\"Shape of Train Combined Embeddings:\", train_combined_embeddings.shape)\n",
        "# print(\"Shape of Test Combined Embeddings:\", test_combined_embeddings.shape)\n",
        "print(\"Sample Main Role Label:\", train_data[\"main_role_encoded\"].iloc[0])\n",
        "print(\"Sample Fine-Grained Role Label:\", train_data[\"fine_roles_encoded\"].iloc[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soFtxOxtOb0h",
        "outputId": "6ee09aa8-1a30-4826-d847-71427882f8b6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['Forgotten'] will be ignored\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Main Role Encoder Classes: ['Antagonist' 'Innocent' 'Protagonist']\n",
            "Fine Role Classes: ['Bigot' 'Conspirator' 'Corrupt' 'Deceiver' 'Exploited'\n",
            " 'Foreign Adversary' 'Guardian' 'Incompetent' 'Instigator' 'Martyr'\n",
            " 'Peacemaker' 'Rebel' 'Saboteur' 'Scapegoat' 'Spy' 'Terrorist' 'Traitor'\n",
            " 'Tyrant' 'Underdog' 'Victim' 'Virtuous']\n",
            "Sample Main Role Label: 0\n",
            "Sample Fine-Grained Role Label: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for Main Role Classifier"
      ],
      "metadata": {
        "id": "gwo9F3xlPROe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "from torch.optim import AdamW\n",
        "from torch.nn import CrossEntropyLoss\n",
        "import torch\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Define the main role classification model\n",
        "main_role_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"microsoft/deberta-base\", num_labels=3  # Three main roles\n",
        ")\n",
        "main_role_model.to(device)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = AdamW(main_role_model.parameters(), lr=5e-5)\n",
        "\n",
        "# Compute class weights for main role classification\n",
        "class_weights_main = compute_class_weight(\n",
        "    class_weight=\"balanced\",\n",
        "    classes=np.unique(train_data[\"main_role_encoded\"]),\n",
        "    y=train_data[\"main_role_encoded\"]\n",
        ")\n",
        "class_weights_main = torch.tensor(class_weights_main, dtype=torch.float).to(device)\n",
        "\n",
        "# Update loss function for main role\n",
        "loss_fn = CrossEntropyLoss(weight=class_weights_main)\n",
        "\n",
        "# Training loop for main role classifier with gradient clipping\n",
        "# Training loop for main role classifier with gradient clipping and early stopping\n",
        "def train_main_role_model(model, train_inputs, train_labels, epochs=1, clip_norm=1.5, loss_threshold=0.01):\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for i in range(0, len(train_inputs[\"input_ids\"]), 16):  # Batch size of 32\n",
        "            # Prepare inputs and labels for the current batch\n",
        "            input_ids = train_inputs[\"input_ids\"][i:i+16].to(device)\n",
        "            attention_mask = train_inputs[\"attention_mask\"][i:i+16].to(device)\n",
        "            labels = torch.tensor(train_labels[i:i+16], dtype=torch.long).to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)  # Pass labels here\n",
        "            loss = outputs.loss  # Compute loss\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_norm)\n",
        "\n",
        "            # Optimizer step\n",
        "            optimizer.step()\n",
        "\n",
        "        # Compute average loss for the epoch\n",
        "        avg_loss = total_loss / len(train_inputs[\"input_ids\"])\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {avg_loss}\")\n",
        "\n",
        "        # Check if the average loss is below the threshold\n",
        "        if avg_loss < loss_threshold:\n",
        "            print(f\"Early stopping triggered at epoch {epoch + 1}. Loss: {avg_loss:.4f}\")\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "# Train the main role model\n",
        "train_labels_main = train_data[\"main_role_encoded\"].tolist()\n",
        "train_main_role_model(main_role_model, train_inputs, train_labels_main, epochs=30, clip_norm=3.5, loss_threshold=0.0001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpz5QdswPSK1",
        "outputId": "888bf5ad-ba6e-481b-81ed-7fb75c0725d8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.05546349036867601\n",
            "Epoch 2, Loss: 0.04655283172852802\n",
            "Epoch 3, Loss: 0.032691068158750114\n",
            "Epoch 4, Loss: 0.020891856769249387\n",
            "Epoch 5, Loss: 0.007741831067990321\n",
            "Epoch 6, Loss: 0.003440674956264246\n",
            "Epoch 7, Loss: 0.001483241932289879\n",
            "Epoch 8, Loss: 0.003059934401106307\n",
            "Epoch 9, Loss: 0.002701587474205613\n",
            "Epoch 10, Loss: 0.0016885458153543355\n",
            "Epoch 11, Loss: 0.00023886056835287298\n",
            "Epoch 12, Loss: 0.0013916754778487315\n",
            "Epoch 13, Loss: 0.0037094761916380173\n",
            "Epoch 14, Loss: 0.0015640445998389729\n",
            "Epoch 15, Loss: 0.000831133104195006\n",
            "Epoch 16, Loss: 0.00047147688375153736\n",
            "Epoch 17, Loss: 4.4469326589236134e-05\n",
            "Early stopping triggered at epoch 17. Loss: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for Fine-Grained Role Classifiers"
      ],
      "metadata": {
        "id": "W6P4TJHtPWhs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from torch.nn import BCEWithLogitsLoss\n",
        "from torch.optim import AdamW\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, hamming_loss\n",
        "\n",
        "# Define the transformer model for fine-grained classification\n",
        "fine_grained_model_name = \"bert-base-uncased\"  # Change to any transformer model you prefer\n",
        "fine_grained_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    fine_grained_model_name, num_labels=len(fine_role_binarizer.classes_)\n",
        ")\n",
        "fine_grained_model.to(device)\n",
        "\n",
        "# Define tokenizer\n",
        "fine_grained_tokenizer = AutoTokenizer.from_pretrained(fine_grained_model_name)\n",
        "\n",
        "# Tokenize context for fine-grained roles\n",
        "def tokenize_context(data, tokenizer):\n",
        "    return tokenizer(\n",
        "        data[\"context\"].tolist(),\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "# Tokenize the data\n",
        "train_inputs_fine = tokenize_context(train_data, fine_grained_tokenizer)\n",
        "test_inputs_fine = tokenize_context(test_data, fine_grained_tokenizer)\n",
        "\n",
        "# Loss function for multi-label classification\n",
        "loss_fn_fine = BCEWithLogitsLoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer_fine = AdamW(fine_grained_model.parameters(), lr=5e-5)\n",
        "\n",
        "# Training loop for fine-grained classification with loss threshold\n",
        "def train_fine_grained_model(model, train_inputs, train_labels, epochs=3, batch_size=16, clip_norm=1.0, loss_threshold=0.01):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for i in range(0, len(train_inputs[\"input_ids\"]), batch_size):\n",
        "            input_ids = train_inputs[\"input_ids\"][i:i+batch_size].to(device)\n",
        "            attention_mask = train_inputs[\"attention_mask\"][i:i+batch_size].to(device)\n",
        "            labels = torch.tensor(train_labels[i:i+batch_size], dtype=torch.float).to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            optimizer_fine.zero_grad()\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            loss = loss_fn_fine(logits, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_norm)\n",
        "\n",
        "            # Optimizer step\n",
        "            optimizer_fine.step()\n",
        "\n",
        "        avg_loss = total_loss / len(train_inputs[\"input_ids\"])\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {avg_loss:.5f}\")\n",
        "\n",
        "        # Stop training if loss falls below the threshold\n",
        "        if avg_loss < loss_threshold:\n",
        "            print(f\"Stopping early at Epoch {epoch + 1} as loss {avg_loss:.4f} is below the threshold {loss_threshold:.4f}\")\n",
        "            break\n",
        "\n",
        "# Evaluate the fine-grained model\n",
        "def evaluate_fine_grained_model(model, test_inputs, test_labels, binarizer):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(test_inputs[\"input_ids\"]), 16):  # Batch size of 32\n",
        "            input_ids = test_inputs[\"input_ids\"][i:i+16].to(device)\n",
        "            attention_mask = test_inputs[\"attention_mask\"][i:i+16].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            probs = torch.sigmoid(logits).cpu().numpy()  # Sigmoid for multi-label classification\n",
        "            predictions.extend(probs)\n",
        "\n",
        "    predictions = np.array(predictions)\n",
        "    predictions_binary = (predictions > 0.5).astype(int)  # Convert probabilities to binary labels\n",
        "\n",
        "    # Exact Match Ratio (multi-label accuracy)\n",
        "    exact_match_ratio = np.mean([np.array_equal(true, pred) for true, pred in zip(test_labels, predictions_binary)])\n",
        "    print(f\"\\nExact Match Ratio (Fine-Grained Roles): {exact_match_ratio:.4f}\")\n",
        "\n",
        "    # Hamming Loss\n",
        "    hamming = hamming_loss(test_labels, predictions_binary)\n",
        "    print(f\"Hamming Loss: {hamming:.4f}\")\n",
        "\n",
        "    # Classification Report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(test_labels, predictions_binary, target_names=binarizer.classes_))\n",
        "\n",
        "    # Convert predictions back to label format\n",
        "    true_labels = binarizer.inverse_transform(test_labels)\n",
        "    predicted_labels = binarizer.inverse_transform(predictions_binary)\n",
        "\n",
        "    return true_labels, predicted_labels, exact_match_ratio, hamming\n",
        "\n",
        "# Prepare data for training and evaluation\n",
        "train_labels_fine = np.array(train_data[\"fine_roles_encoded\"].tolist())\n",
        "test_labels_fine = np.array(test_data[\"fine_roles_encoded\"].tolist())\n",
        "\n",
        "# Train the fine-grained classification model\n",
        "train_fine_grained_model(\n",
        "    fine_grained_model,\n",
        "    train_inputs_fine,\n",
        "    train_labels_fine,\n",
        "    epochs=30,\n",
        "    batch_size=16,\n",
        "    clip_norm=3.5,\n",
        "    loss_threshold=0.001  # Define a threshold for early stopping\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "true_fine_roles, predicted_fine_roles, exact_match_ratio, hamming_loss_val = evaluate_fine_grained_model(\n",
        "    fine_grained_model, test_inputs_fine, test_labels_fine, fine_role_binarizer\n",
        ")\n",
        "\n",
        "# Display true and predicted roles\n",
        "# print(\"\\nTrue Fine-Grained Roles vs Predicted Fine-Grained Roles:\")\n",
        "# for true, pred in zip(true_fine_roles, predicted_fine_roles):\n",
        "#     print(f\"True: {true}, Predicted: {pred}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-2sjy4ybkd8",
        "outputId": "d35fd9bd-7d13-43de-9110-5ea5e548b82a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.02435\n",
            "Epoch 2, Loss: 0.01350\n",
            "Epoch 3, Loss: 0.01253\n",
            "Epoch 4, Loss: 0.01224\n",
            "Epoch 5, Loss: 0.01187\n",
            "Epoch 6, Loss: 0.01132\n",
            "Epoch 7, Loss: 0.01089\n",
            "Epoch 8, Loss: 0.01049\n",
            "Epoch 9, Loss: 0.00986\n",
            "Epoch 10, Loss: 0.00888\n",
            "Epoch 11, Loss: 0.00813\n",
            "Epoch 12, Loss: 0.00719\n",
            "Epoch 13, Loss: 0.00617\n",
            "Epoch 14, Loss: 0.00522\n",
            "Epoch 15, Loss: 0.00440\n",
            "Epoch 16, Loss: 0.00367\n",
            "Epoch 17, Loss: 0.00322\n",
            "Epoch 18, Loss: 0.00281\n",
            "Epoch 19, Loss: 0.00245\n",
            "Epoch 20, Loss: 0.00217\n",
            "Epoch 21, Loss: 0.00191\n",
            "Epoch 22, Loss: 0.00178\n",
            "Epoch 23, Loss: 0.00157\n",
            "Epoch 24, Loss: 0.00143\n",
            "Epoch 25, Loss: 0.00131\n",
            "Epoch 26, Loss: 0.00122\n",
            "Epoch 27, Loss: 0.00112\n",
            "Epoch 28, Loss: 0.00104\n",
            "Epoch 29, Loss: 0.00095\n",
            "Stopping early at Epoch 29 as loss 0.0009 is below the threshold 0.0010\n",
            "\n",
            "Exact Match Ratio (Fine-Grained Roles): 0.3116\n",
            "Hamming Loss: 0.0452\n",
            "\n",
            "Classification Report:\n",
            "                   precision    recall  f1-score   support\n",
            "\n",
            "            Bigot       0.50      0.50      0.50         2\n",
            "      Conspirator       0.83      0.59      0.69        17\n",
            "          Corrupt       0.45      0.36      0.40        14\n",
            "         Deceiver       0.00      0.00      0.00        14\n",
            "        Exploited       0.00      0.00      0.00         0\n",
            "Foreign Adversary       0.67      0.57      0.62         7\n",
            "         Guardian       0.00      0.00      0.00         6\n",
            "      Incompetent       0.40      0.15      0.22        13\n",
            "       Instigator       0.56      0.45      0.50        11\n",
            "           Martyr       1.00      0.20      0.33         5\n",
            "       Peacemaker       1.00      0.60      0.75         5\n",
            "            Rebel       0.00      0.00      0.00         3\n",
            "         Saboteur       1.00      0.75      0.86         4\n",
            "        Scapegoat       0.00      0.00      0.00         2\n",
            "              Spy       0.00      0.00      0.00         0\n",
            "        Terrorist       1.00      0.75      0.86         4\n",
            "          Traitor       0.00      0.00      0.00         4\n",
            "           Tyrant       0.80      0.27      0.40        15\n",
            "         Underdog       0.00      0.00      0.00         3\n",
            "           Victim       0.55      0.50      0.52        12\n",
            "         Virtuous       0.00      0.00      0.00         5\n",
            "\n",
            "        micro avg       0.59      0.32      0.42       146\n",
            "        macro avg       0.42      0.27      0.32       146\n",
            "     weighted avg       0.51      0.32      0.38       146\n",
            "      samples avg       0.32      0.32      0.32       146\n",
            "\n",
            "\n",
            "True Fine-Grained Roles vs Predicted Fine-Grained Roles:\n",
            "True: ('Foreign Adversary',), Predicted: ('Foreign Adversary',)\n",
            "True: ('Corrupt',), Predicted: ('Bigot',)\n",
            "True: ('Conspirator',), Predicted: ('Conspirator',)\n",
            "True: ('Tyrant',), Predicted: ('Corrupt',)\n",
            "True: ('Corrupt',), Predicted: ('Corrupt',)\n",
            "True: ('Deceiver',), Predicted: ('Corrupt',)\n",
            "True: ('Foreign Adversary',), Predicted: ()\n",
            "True: ('Deceiver',), Predicted: ()\n",
            "True: ('Conspirator',), Predicted: ()\n",
            "True: ('Instigator',), Predicted: ()\n",
            "True: ('Conspirator',), Predicted: ('Conspirator',)\n",
            "True: ('Corrupt', 'Deceiver'), Predicted: ()\n",
            "True: ('Instigator',), Predicted: ()\n",
            "True: ('Instigator',), Predicted: ()\n",
            "True: ('Victim',), Predicted: ('Victim',)\n",
            "True: ('Tyrant',), Predicted: ('Instigator',)\n",
            "True: ('Foreign Adversary',), Predicted: ('Foreign Adversary',)\n",
            "True: ('Incompetent',), Predicted: ()\n",
            "True: ('Peacemaker',), Predicted: ()\n",
            "True: ('Instigator',), Predicted: ('Instigator', 'Tyrant')\n",
            "True: ('Deceiver',), Predicted: ('Corrupt',)\n",
            "True: ('Conspirator', 'Saboteur'), Predicted: ('Conspirator', 'Saboteur')\n",
            "True: ('Incompetent',), Predicted: ()\n",
            "True: ('Rebel',), Predicted: ('Underdog',)\n",
            "True: ('Incompetent',), Predicted: ()\n",
            "True: ('Virtuous',), Predicted: ()\n",
            "True: ('Terrorist',), Predicted: ()\n",
            "True: ('Foreign Adversary',), Predicted: ('Foreign Adversary',)\n",
            "True: ('Terrorist',), Predicted: ('Terrorist',)\n",
            "True: ('Tyrant',), Predicted: ('Tyrant',)\n",
            "True: ('Victim',), Predicted: ('Victim',)\n",
            "True: ('Conspirator',), Predicted: ('Conspirator',)\n",
            "True: ('Incompetent',), Predicted: ()\n",
            "True: ('Martyr',), Predicted: ()\n",
            "True: (), Predicted: ('Incompetent',)\n",
            "True: ('Conspirator',), Predicted: ('Deceiver',)\n",
            "True: ('Incompetent',), Predicted: ('Incompetent',)\n",
            "True: ('Tyrant',), Predicted: ()\n",
            "True: ('Corrupt',), Predicted: ('Conspirator',)\n",
            "True: ('Traitor',), Predicted: ()\n",
            "True: ('Scapegoat',), Predicted: ()\n",
            "True: ('Guardian',), Predicted: ('Foreign Adversary',)\n",
            "True: ('Conspirator',), Predicted: ('Conspirator',)\n",
            "True: ('Corrupt',), Predicted: ('Victim',)\n",
            "True: ('Victim',), Predicted: ('Victim',)\n",
            "True: ('Deceiver',), Predicted: ('Corrupt',)\n",
            "True: ('Conspirator',), Predicted: ()\n",
            "True: ('Deceiver',), Predicted: ()\n",
            "True: ('Tyrant',), Predicted: ()\n",
            "True: ('Corrupt',), Predicted: ()\n",
            "True: ('Underdog',), Predicted: ('Victim',)\n",
            "True: ('Virtuous',), Predicted: ()\n",
            "True: ('Deceiver',), Predicted: ('Conspirator',)\n",
            "True: ('Instigator',), Predicted: ()\n",
            "True: ('Guardian',), Predicted: ('Instigator',)\n",
            "True: ('Deceiver',), Predicted: ('Incompetent',)\n",
            "True: ('Conspirator',), Predicted: ('Guardian',)\n",
            "True: ('Incompetent',), Predicted: ()\n",
            "True: ('Rebel',), Predicted: ('Underdog',)\n",
            "True: ('Peacemaker', 'Virtuous'), Predicted: ('Peacemaker',)\n",
            "True: ('Incompetent',), Predicted: ('Incompetent',)\n",
            "True: ('Conspirator', 'Corrupt'), Predicted: ()\n",
            "True: ('Saboteur',), Predicted: ('Saboteur',)\n",
            "True: ('Instigator',), Predicted: ()\n",
            "True: ('Tyrant',), Predicted: ('Tyrant',)\n",
            "True: ('Martyr',), Predicted: ('Victim',)\n",
            "True: ('Corrupt',), Predicted: ('Corrupt',)\n",
            "True: ('Incompetent',), Predicted: ('Scapegoat',)\n",
            "True: ('Guardian',), Predicted: ()\n",
            "True: ('Corrupt',), Predicted: ('Corrupt',)\n",
            "True: ('Conspirator', 'Tyrant'), Predicted: ('Conspirator', 'Tyrant')\n",
            "True: ('Underdog',), Predicted: ('Incompetent',)\n",
            "True: ('Tyrant',), Predicted: ()\n",
            "True: ('Terrorist',), Predicted: ('Terrorist',)\n",
            "True: ('Instigator',), Predicted: ()\n",
            "True: ('Virtuous',), Predicted: ()\n",
            "True: ('Victim',), Predicted: ('Victim',)\n",
            "True: ('Deceiver', 'Incompetent'), Predicted: ('Corrupt',)\n",
            "True: ('Victim',), Predicted: ()\n",
            "True: ('Incompetent',), Predicted: ('Victim',)\n",
            "True: ('Virtuous',), Predicted: ()\n",
            "True: ('Conspirator',), Predicted: ('Conspirator',)\n",
            "True: ('Conspirator',), Predicted: ()\n",
            "True: ('Corrupt', 'Deceiver'), Predicted: ()\n",
            "True: ('Bigot',), Predicted: ('Corrupt',)\n",
            "True: ('Victim',), Predicted: ()\n",
            "True: ('Guardian',), Predicted: ('Virtuous',)\n",
            "True: ('Victim',), Predicted: ('Victim',)\n",
            "True: ('Foreign Adversary',), Predicted: ()\n",
            "True: ('Victim',), Predicted: ()\n",
            "True: ('Victim',), Predicted: ()\n",
            "True: ('Martyr',), Predicted: ()\n",
            "True: (), Predicted: ()\n",
            "True: ('Foreign Adversary',), Predicted: ()\n",
            "True: ('Terrorist',), Predicted: ('Terrorist',)\n",
            "True: ('Deceiver',), Predicted: ()\n",
            "True: ('Scapegoat',), Predicted: ()\n",
            "True: ('Incompetent',), Predicted: ()\n",
            "True: ('Bigot',), Predicted: ('Bigot',)\n",
            "True: ('Instigator',), Predicted: ('Instigator',)\n",
            "True: ('Conspirator',), Predicted: ('Conspirator',)\n",
            "True: ('Peacemaker',), Predicted: ('Instigator',)\n",
            "True: ('Corrupt',), Predicted: ()\n",
            "True: ('Conspirator',), Predicted: ('Conspirator',)\n",
            "True: ('Conspirator',), Predicted: ('Conspirator',)\n",
            "True: ('Martyr',), Predicted: ()\n",
            "True: ('Tyrant',), Predicted: ()\n",
            "True: ('Underdog',), Predicted: ()\n",
            "True: ('Instigator',), Predicted: ('Instigator',)\n",
            "True: ('Victim',), Predicted: ('Victim',)\n",
            "True: ('Deceiver',), Predicted: ()\n",
            "True: ('Victim',), Predicted: ()\n",
            "True: ('Corrupt',), Predicted: ('Corrupt',)\n",
            "True: ('Deceiver',), Predicted: ()\n",
            "True: ('Corrupt', 'Traitor'), Predicted: ('Victim',)\n",
            "True: ('Peacemaker',), Predicted: ('Peacemaker',)\n",
            "True: ('Tyrant',), Predicted: ('Tyrant',)\n",
            "True: ('Victim',), Predicted: ()\n",
            "True: ('Tyrant',), Predicted: ()\n",
            "True: ('Guardian',), Predicted: ('Foreign Adversary',)\n",
            "True: ('Saboteur',), Predicted: ()\n",
            "True: ('Peacemaker',), Predicted: ('Peacemaker',)\n",
            "True: ('Corrupt',), Predicted: ('Corrupt',)\n",
            "True: ('Traitor',), Predicted: ()\n",
            "True: ('Conspirator', 'Saboteur'), Predicted: ('Saboteur',)\n",
            "True: ('Foreign Adversary',), Predicted: ('Foreign Adversary',)\n",
            "True: ('Instigator',), Predicted: ('Instigator',)\n",
            "True: ('Rebel',), Predicted: ()\n",
            "True: ('Martyr',), Predicted: ('Martyr',)\n",
            "True: ('Guardian',), Predicted: ('Underdog',)\n",
            "True: ('Tyrant',), Predicted: ()\n",
            "True: ('Tyrant',), Predicted: ()\n",
            "True: ('Deceiver',), Predicted: ()\n",
            "True: ('Tyrant',), Predicted: ()\n",
            "True: ('Tyrant',), Predicted: ('Instigator',)\n",
            "True: ('Incompetent',), Predicted: ('Traitor',)\n",
            "True: ('Incompetent', 'Traitor'), Predicted: ()\n",
            "True: ('Instigator',), Predicted: ('Instigator',)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pmn00rwfPXDd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation"
      ],
      "metadata": {
        "id": "MRFAzMe8PcV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score, hamming_loss\n",
        "import torch\n",
        "\n",
        "# Evaluate main role classifier with batch-wise evaluation\n",
        "def evaluate_main_role_model_batchwise(model, test_inputs, test_labels, batch_size=16):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(test_inputs[\"input_ids\"]), batch_size):\n",
        "            input_ids = test_inputs[\"input_ids\"][i:i+batch_size].to(device)\n",
        "            attention_mask = test_inputs[\"attention_mask\"][i:i+batch_size].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            batch_predictions = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
        "            predictions.extend(batch_predictions)\n",
        "\n",
        "    predictions = np.array(predictions)\n",
        "\n",
        "    # Exact Match Ratio for main roles (accuracy)\n",
        "    exact_match_ratio = accuracy_score(test_labels, predictions)\n",
        "    print(f\"Exact Match Ratio (Main Role): {exact_match_ratio:.4f}\")\n",
        "\n",
        "    # Classification Report\n",
        "    print(classification_report(test_labels, predictions, target_names=main_role_encoder.classes_))\n",
        "\n",
        "    # Display True and Predicted Main Roles\n",
        "    true_main_roles = main_role_encoder.inverse_transform(test_labels)\n",
        "    predicted_main_roles = main_role_encoder.inverse_transform(predictions)\n",
        "    # print(\"\\nTrue Main Roles vs Predicted Main Roles:\")\n",
        "    # for true, pred in zip(true_main_roles, predicted_main_roles):\n",
        "    #     print(f\"True: {true}, Predicted: {pred}\")\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# Evaluate Fine-Grained Classifiers with batch-wise evaluation\n",
        "test_main_predictions = evaluate_main_role_model_batchwise(\n",
        "    main_role_model, test_inputs, test_data[\"main_role_encoded\"].tolist(), batch_size=16\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Evaluate fine-grained classifiers\n",
        "# def evaluate_fine_grained_classifiers_by_model(classifiers_by_model, test_features, test_labels, test_main_predictions, taxonomy):\n",
        "#     results = {}\n",
        "#     for model_name, classifiers in classifiers_by_model.items():\n",
        "#         print(f\"\\nEvaluating fine-grained classifiers using {model_name}...\")\n",
        "#         predictions = []\n",
        "\n",
        "#         for i, main_role in enumerate(test_main_predictions):\n",
        "#             classifier = classifiers[main_role_encoder.inverse_transform([main_role])[0]]\n",
        "#             probs = classifier.predict_proba(test_features[i].reshape(1, -1))[0]\n",
        "#             enforced_probs = enforce_taxonomy(main_role, probs, taxonomy, fine_role_binarizer)\n",
        "#             predictions.append((enforced_probs > 0.17).astype(int))\n",
        "\n",
        "#         predictions = np.array(predictions)\n",
        "\n",
        "#         # Calculate metrics\n",
        "#         exact_match_ratio = np.mean([np.array_equal(true, pred) for true, pred in zip(test_labels, predictions)])\n",
        "#         hl = hamming_loss(test_labels, predictions)\n",
        "#         report = classification_report(test_labels, predictions, target_names=fine_role_binarizer.classes_)\n",
        "\n",
        "#         results[model_name] = {\n",
        "#             \"Exact Match Ratio\": exact_match_ratio,\n",
        "#             \"Hamming Loss\": hl,\n",
        "#             \"Classification Report\": report,\n",
        "#         }\n",
        "\n",
        "#         print(f\"\\nExact Match Ratio (Fine-Grained Roles): {exact_match_ratio:.4f}\")\n",
        "#         print(f\"Hamming Loss: {hl:.4f}\")\n",
        "#         print(report)\n",
        "\n",
        "#     return results\n",
        "\n",
        "\n",
        "\n",
        "# # Evaluate Main Role Classifier\n",
        "# test_main_predictions = evaluate_main_role_model(\n",
        "#     main_role_model, test_inputs, test_data[\"main_role_encoded\"].tolist()\n",
        "# )\n",
        "\n",
        "# # Extract Features for Fine-Grained Classifiers\n",
        "# test_features = extract_features(test_inputs, main_role_model)\n",
        "\n",
        "# fine_grained_results = evaluate_fine_grained_classifiers_by_model(\n",
        "#     fine_grained_classifiers_by_model,\n",
        "#     test_features,\n",
        "#     np.array(test_data[\"fine_roles_encoded\"].tolist()),\n",
        "#     test_main_predictions,\n",
        "#     ENTITY_TAXONOMY\n",
        "# )\n",
        "\n",
        "# # Display the best-performing model\n",
        "# best_model = max(fine_grained_results, key=lambda model: fine_grained_results[model][\"Exact Match Ratio\"])\n",
        "# print(f\"\\nBest Model: {best_model}\")\n",
        "# print(f\"Exact Match Ratio: {fine_grained_results[best_model]['Exact Match Ratio']:.4f}\")\n",
        "# print(f\"Hamming Loss: {fine_grained_results[best_model]['Hamming Loss']:.4f}\")\n",
        "# print(f\"Classification Report:\\n{fine_grained_results[best_model]['Classification Report']}\")\n"
      ],
      "metadata": {
        "id": "_o4qQzupPc_c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4129b570-93a2-47d3-b0aa-9ea3348c2451"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exact Match Ratio (Main Role): 0.8043\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Antagonist       0.83      0.95      0.89        96\n",
            "    Innocent       0.67      0.50      0.57        16\n",
            " Protagonist       0.71      0.46      0.56        26\n",
            "\n",
            "    accuracy                           0.80       138\n",
            "   macro avg       0.74      0.64      0.67       138\n",
            "weighted avg       0.79      0.80      0.79       138\n",
            "\n"
          ]
        }
      ]
    }
  ]
}