{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Load Annotations and Extract Entity Mentions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       start_offset   end_offset\n",
      "count    414.000000   414.000000\n",
      "mean    1343.722222  1352.758454\n",
      "std      976.379227   976.200424\n",
      "min       55.000000    64.000000\n",
      "25%      489.750000   495.500000\n",
      "50%     1221.000000  1230.000000\n",
      "75%     2063.500000  2068.750000\n",
      "max     4909.000000  4920.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Custom parser function to handle fine-grained roles\n",
    "def parse_annotations(line):\n",
    "    parts = line.strip().split(\"\\t\")  # Split the line by tabs\n",
    "    article_id, entity_mention, start_offset, end_offset, main_role = parts[:5]\n",
    "    fine_grained_roles = parts[5:]  # Capture remaining parts as fine-grained roles\n",
    "    return {\n",
    "        \"article_id\": article_id,\n",
    "        \"entity_mention\": entity_mention,\n",
    "        \"start_offset\": int(start_offset),\n",
    "        \"end_offset\": int(end_offset),\n",
    "        \"main_role\": main_role,\n",
    "        \"fine_grained_roles\": fine_grained_roles\n",
    "    }\n",
    "\n",
    "# Read the file and parse it line-by-line\n",
    "with open(\"subtask-1-annotations.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = [parse_annotations(line) for line in f]\n",
    "\n",
    "# Convert to DataFrame\n",
    "annotations = pd.DataFrame(data)\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(annotations.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Generate Label Encoders</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         article_id entity_mention  start_offset  end_offset    main_role  \\\n",
      "0  EN_UA_103861.txt        Chinese           791         797   Antagonist   \n",
      "1  EN_UA_103861.txt          China          1516        1520   Antagonist   \n",
      "2  EN_UA_103861.txt          Hamas          2121        2125   Antagonist   \n",
      "3  EN_UA_103861.txt   Donald Trump          4909        4920  Protagonist   \n",
      "4  EN_UA_021270.txt         Yermak           667         672   Antagonist   \n",
      "\n",
      "       fine_grained_roles  Antagonist  Innocent  Protagonist  Bigot  ...  \\\n",
      "0                   [Spy]           1         0            0      0  ...   \n",
      "1            [Instigator]           1         0            0      0  ...   \n",
      "2             [Terrorist]           1         0            0      0  ...   \n",
      "3  [Peacemaker, Guardian]           0         0            1      0  ...   \n",
      "4           [Incompetent]           1         0            0      0  ...   \n",
      "\n",
      "   Rebel  Saboteur  Scapegoat  Spy  Terrorist  Traitor  Tyrant  Underdog  \\\n",
      "0      0         0          0    1          0        0       0         0   \n",
      "1      0         0          0    0          0        0       0         0   \n",
      "2      0         0          0    0          1        0       0         0   \n",
      "3      0         0          0    0          0        0       0         0   \n",
      "4      0         0          0    0          0        0       0         0   \n",
      "\n",
      "   Victim  Virtuous  \n",
      "0       0         0  \n",
      "1       0         0  \n",
      "2       0         0  \n",
      "3       0         0  \n",
      "4       0         0  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Create label encoders for main roles and fine-grained roles\n",
    "main_role_encoder = MultiLabelBinarizer()\n",
    "fine_grained_role_encoder = MultiLabelBinarizer()\n",
    "\n",
    "# Fit the encoders on the roles found in the dataset\n",
    "main_role_labels = annotations[\"main_role\"].apply(lambda x: [x])  # Wrap each label in a list\n",
    "fine_grained_labels = annotations[\"fine_grained_roles\"]\n",
    "\n",
    "main_role_encoded = main_role_encoder.fit_transform(main_role_labels)\n",
    "fine_grained_encoded = fine_grained_role_encoder.fit_transform(fine_grained_labels)\n",
    "\n",
    "# Convert encoded labels back to DataFrames for easy merging\n",
    "main_role_df = pd.DataFrame(main_role_encoded, columns=main_role_encoder.classes_)\n",
    "fine_grained_role_df = pd.DataFrame(fine_grained_encoded, columns=fine_grained_role_encoder.classes_)\n",
    "\n",
    "# Merge the encoded labels back into the original DataFrame\n",
    "annotations = pd.concat([annotations, main_role_df, fine_grained_role_df], axis=1)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(annotations.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Extract Context Sentences</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         article_id entity_mention  start_offset  end_offset    main_role  \\\n",
      "0  EN_UA_103861.txt        Chinese           791         797   Antagonist   \n",
      "1  EN_UA_103861.txt          China          1516        1520   Antagonist   \n",
      "2  EN_UA_103861.txt          Hamas          2121        2125   Antagonist   \n",
      "3  EN_UA_103861.txt   Donald Trump          4909        4920  Protagonist   \n",
      "4  EN_UA_021270.txt         Yermak           667         672   Antagonist   \n",
      "\n",
      "       fine_grained_roles  Antagonist  Innocent  Protagonist  Bigot  ...  \\\n",
      "0                   [Spy]           1         0            0      0  ...   \n",
      "1            [Instigator]           1         0            0      0  ...   \n",
      "2             [Terrorist]           1         0            0      0  ...   \n",
      "3  [Peacemaker, Guardian]           0         0            1      0  ...   \n",
      "4           [Incompetent]           1         0            0      0  ...   \n",
      "\n",
      "   Saboteur  Scapegoat  Spy  Terrorist  Traitor  Tyrant  Underdog  Victim  \\\n",
      "0         0          0    1          0        0       0         0       0   \n",
      "1         0          0    0          0        0       0         0       0   \n",
      "2         0          0    0          1        0       0         0       0   \n",
      "3         0          0    0          0        0       0         0       0   \n",
      "4         0          0    0          0        0       0         0       0   \n",
      "\n",
      "   Virtuous                                            context  \n",
      "0         0  en an astounding 6,300% increase in the number...  \n",
      "1         0  States to monitor their citizens in our countr...  \n",
      "2         0   to Ukraine.\\n\\nOn October 7, Israel was invad...  \n",
      "3         0   it is useful to remember what was happening w...  \n",
      "4         0  he hawkish DC-based Hudson Institute think tan...  \n",
      "\n",
      "[5 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "# Helper function to extract the sentence containing the entity\n",
    "def extract_entity_context(article_path, start_offset, end_offset):\n",
    "    with open(article_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Extract the surrounding sentence or context\n",
    "    entity_span = text[start_offset:end_offset]\n",
    "    sentence = text[max(0, start_offset - 50):end_offset + 50]  # Surrounding context\n",
    "    return entity_span, sentence\n",
    "\n",
    "# Extract context for all rows in the annotations DataFrame\n",
    "context_data = []\n",
    "for _, row in annotations.iterrows():\n",
    "    article_path = f\"raw-documents/{row['article_id']}\"  # Adjust path as needed\n",
    "    entity_span, context = extract_entity_context(article_path, row['start_offset'], row['end_offset'])\n",
    "    context_data.append(context)\n",
    "\n",
    "# Add the extracted context to the DataFrame\n",
    "annotations[\"context\"] = context_data\n",
    "\n",
    "# Display the DataFrame with the context column\n",
    "print(annotations.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tokenization</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      attention_mask  \\\n",
      "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "\n",
      "                                           input_ids  \n",
      "0  [0, 22, 142, 237, 16450, 33770, 305, 4, 7739, ...  \n",
      "1  [0, 46684, 47, 12983, 2363, 130367, 7, 23, 244...  \n",
      "2  [0, 47, 82739, 5, 2161, 18374, 361, 4, 8254, 5...  \n",
      "3  [0, 442, 83, 80234, 47, 37629, 2367, 509, 1230...  \n",
      "4  [0, 764, 6, 187404, 4745, 31455, 9, 77007, 215...  \n"
     ]
    }
   ],
   "source": [
    "from transformers import XLMRobertaTokenizer\n",
    "\n",
    "# Load XLM-RoBERTa tokenizer\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "# Tokenize the context sentences\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"context\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Prepare the dataset for tokenization\n",
    "dataset = annotations[[\"context\"]]\n",
    "tokenized_dataset = dataset.apply(lambda x: tokenize_function(x), axis=1)\n",
    "\n",
    "# Convert tokenized data to DataFrame for model input\n",
    "tokenized_df = pd.DataFrame(list(tokenized_dataset))\n",
    "print(tokenized_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Prepare Dataset for Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\talee\\AppData\\Local\\Temp\\ipykernel_34460\\3743574493.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.main_labels = torch.tensor(main_labels, dtype=torch.float32)\n",
      "C:\\Users\\talee\\AppData\\Local\\Temp\\ipykernel_34460\\3743574493.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.fine_labels = torch.tensor(fine_labels, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class EntityFramingDataset(Dataset):\n",
    "    def __init__(self, encodings, main_labels, fine_labels):\n",
    "        # Convert lists to tensors with appropriate dimensions\n",
    "        self.encodings = encodings\n",
    "        self.main_labels = torch.tensor(main_labels, dtype=torch.float32)\n",
    "        self.fine_labels = torch.tensor(fine_labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.main_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Prepare the input encoding and labels for the given index\n",
    "        item = {key: torch.tensor(val[idx]).clone().detach() for key, val in self.encodings.items()}\n",
    "        item[\"main_labels\"] = self.main_labels[idx]\n",
    "        item[\"fine_labels\"] = self.fine_labels[idx]\n",
    "        return item\n",
    "\n",
    "\n",
    "\n",
    "# Prepare encodings\n",
    "encodings = tokenizer(\n",
    "    list(annotations[\"context\"]),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Prepare labels (ensure tensors are float)\n",
    "main_labels = torch.tensor(main_role_encoded, dtype=torch.float32)\n",
    "fine_labels = torch.tensor(fine_grained_encoded, dtype=torch.float32)\n",
    "\n",
    "# Create the dataset\n",
    "dataset = EntityFramingDataset(encodings, main_labels, fine_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Set Up the Trainer and Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLMRobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Load the XLM-R model for multi-label classification\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained(\n",
    "    \"xlm-roberta-base\",\n",
    "    cache_dir=\"./models\",  # Store/download the model locally\n",
    "    num_labels=len(main_role_encoder.classes_) + len(fine_grained_role_encoder.classes_),\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Define Training Arguments</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\talee\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",            # Directory to save checkpoints\n",
    "    evaluation_strategy=\"epoch\",       # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",             # Save the model at the end of each epoch\n",
    "    logging_dir=\"./logs\",              # Directory for logs\n",
    "    logging_steps=10,                  # Log every 10 steps\n",
    "    per_device_train_batch_size=8,     # Batch size for training\n",
    "    per_device_eval_batch_size=8,      # Batch size for evaluation\n",
    "    num_train_epochs=3,                # Number of epochs\n",
    "    load_best_model_at_end=True,       # Load best model at the end\n",
    "    metric_for_best_model=\"loss\",      # Use loss to select the best model\n",
    "    greater_is_better=False,           # Lower loss is better\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[102], line 21\u001b[0m\n\u001b[0;32m     17\u001b[0m val_fine_labels \u001b[38;5;241m=\u001b[39m [fine_labels[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m val_indices]\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Convert labels to tensors\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Ensure the labels are lists of numerical values, not nested lists\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m train_main_labels_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_main_labels\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m train_fine_labels_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([label \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m train_fine_labels], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     24\u001b[0m val_main_labels_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([label \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m val_main_labels], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[1;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset into 80% train and 20% validation\n",
    "train_indices, val_indices = train_test_split(\n",
    "    list(range(len(encodings['input_ids']))), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create encodings for train and validation sets\n",
    "train_encodings = {key: val[train_indices] for key, val in encodings.items()}\n",
    "val_encodings = {key: val[val_indices] for key, val in encodings.items()}\n",
    "\n",
    "# Create labels for train and validation sets\n",
    "train_main_labels = [main_labels[i] for i in train_indices]\n",
    "val_main_labels = [main_labels[i] for i in val_indices]\n",
    "\n",
    "train_fine_labels = [fine_labels[i] for i in train_indices]\n",
    "val_fine_labels = [fine_labels[i] for i in val_indices]\n",
    "\n",
    "# Convert labels to tensors\n",
    "# Ensure the labels are lists of numerical values, not nested lists\n",
    "train_main_labels_tensor = torch.tensor([label for label in train_main_labels], dtype=torch.float32)\n",
    "train_fine_labels_tensor = torch.tensor([label for label in train_fine_labels], dtype=torch.float32)\n",
    "\n",
    "val_main_labels_tensor = torch.tensor([label for label in val_main_labels], dtype=torch.float32)\n",
    "val_fine_labels_tensor = torch.tensor([label for label in val_fine_labels], dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Create datasets using the custom EntityFramingDataset class\n",
    "train_dataset = EntityFramingDataset(train_encodings, train_main_labels_tensor, train_fine_labels_tensor)\n",
    "eval_dataset = EntityFramingDataset(val_encodings, val_main_labels_tensor, val_fine_labels_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Define Custom Metrics</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    return {\"f1\": f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Initialize the Trainer</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[93], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m      3\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m----> 4\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_dataset\u001b[49m,\n\u001b[0;32m      5\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39meval_dataset,\n\u001b[0;32m      6\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics  \u001b[38;5;66;03m# Attach custom metric\u001b[39;00m\n\u001b[0;32m      7\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics  # Attach custom metric\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Train the Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/156 [10:53<?, ?it/s]\n",
      "  6%|â–‹         | 10/156 [09:27<2:17:59, 56.71s/it]\n",
      "  0%|          | 0/156 [00:00<?, ?it/s]C:\\Users\\talee\\AppData\\Local\\Temp\\ipykernel_34460\\969082892.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]).clone().detach() for key, val in self.encodings.items()}\n",
      "  6%|â–‹         | 10/156 [00:47<10:34,  4.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5866, 'grad_norm': 1.6705063581466675, 'learning_rate': 4.67948717948718e-05, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|â–ˆâ–Ž        | 20/156 [01:34<10:20,  4.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4103, 'grad_norm': 1.0101654529571533, 'learning_rate': 4.358974358974359e-05, 'epoch': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|â–ˆâ–‰        | 30/156 [02:11<07:38,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3242, 'grad_norm': 1.0920099020004272, 'learning_rate': 4.038461538461539e-05, 'epoch': 0.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|â–ˆâ–ˆâ–Œ       | 40/156 [02:47<06:52,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2715, 'grad_norm': 0.4572836458683014, 'learning_rate': 3.717948717948718e-05, 'epoch': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 50/156 [03:25<06:48,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2564, 'grad_norm': 0.45132243633270264, 'learning_rate': 3.397435897435898e-05, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\talee\\AppData\\Local\\Temp\\ipykernel_34460\\969082892.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]).clone().detach() for key, val in self.encodings.items()}\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 60/156 [04:11<06:21,  3.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2341, 'grad_norm': 0.4390738904476166, 'learning_rate': 3.0769230769230774e-05, 'epoch': 1.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 70/156 [04:50<05:22,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2342, 'grad_norm': 0.48095059394836426, 'learning_rate': 2.756410256410257e-05, 'epoch': 1.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 80/156 [05:27<04:56,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2385, 'grad_norm': 0.460703581571579, 'learning_rate': 2.435897435897436e-05, 'epoch': 1.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 90/156 [06:06<04:02,  3.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2264, 'grad_norm': 0.4588775932788849, 'learning_rate': 2.1153846153846154e-05, 'epoch': 1.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 100/156 [06:43<03:43,  3.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2322, 'grad_norm': 0.3887184262275696, 'learning_rate': 1.794871794871795e-05, 'epoch': 1.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\talee\\AppData\\Local\\Temp\\ipykernel_34460\\969082892.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]).clone().detach() for key, val in self.encodings.items()}\n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 110/156 [07:28<02:55,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2267, 'grad_norm': 0.4514479339122772, 'learning_rate': 1.4743589743589745e-05, 'epoch': 2.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 120/156 [08:06<02:12,  3.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2257, 'grad_norm': 0.331670880317688, 'learning_rate': 1.153846153846154e-05, 'epoch': 2.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 130/156 [08:44<01:42,  3.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2302, 'grad_norm': 0.32617467641830444, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 140/156 [09:23<01:00,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2329, 'grad_norm': 0.4229016900062561, 'learning_rate': 5.128205128205128e-06, 'epoch': 2.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 150/156 [10:06<00:25,  4.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2208, 'grad_norm': 0.44514214992523193, 'learning_rate': 1.9230769230769234e-06, 'epoch': 2.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\talee\\AppData\\Local\\Temp\\ipykernel_34460\\969082892.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]).clone().detach() for key, val in self.encodings.items()}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 156/156 [10:45<00:00,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 645.7079, 'train_samples_per_second': 1.923, 'train_steps_per_second': 0.242, 'train_loss': 0.2746143692579025, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=156, training_loss=0.2746143692579025, metrics={'train_runtime': 645.7079, 'train_samples_per_second': 1.923, 'train_steps_per_second': 0.242, 'total_flos': 24258503394792.0, 'train_loss': 0.2746143692579025, 'epoch': 3.0})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Save Model and Results</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./entity-framing-model\\\\tokenizer_config.json',\n",
       " './entity-framing-model\\\\special_tokens_map.json',\n",
       " './entity-framing-model\\\\sentencepiece.bpe.model',\n",
       " './entity-framing-model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./entity-framing-model\")\n",
    "tokenizer.save_pretrained(\"./entity-framing-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Evaluate the Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m train_fine_labels \u001b[38;5;241m=\u001b[39m [fine_labels[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m train_indices]\n\u001b[0;32m     16\u001b[0m val_fine_labels \u001b[38;5;241m=\u001b[39m [fine_labels[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m val_indices]\n\u001b[1;32m---> 18\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mEntityFramingDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_encodings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_main_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_fine_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m eval_dataset \u001b[38;5;241m=\u001b[39m EntityFramingDataset(val_encodings, val_main_labels, val_fine_labels)\n",
      "Cell \u001b[1;32mIn[65], line 7\u001b[0m, in \u001b[0;36mEntityFramingDataset.__init__\u001b[1;34m(self, encodings, main_labels, fine_labels)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, encodings, main_labels, fine_labels):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Convert lists to tensors\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m     main_labels_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     fine_labels_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(fine_labels, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Concatenate the main and fine-grained labels\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset: 80% for training, 20% for evaluation\n",
    "train_indices, val_indices = train_test_split(\n",
    "    range(len(main_labels)), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create training and evaluation datasets\n",
    "train_encodings = {key: val[train_indices] for key, val in encodings.items()}\n",
    "val_encodings = {key: val[val_indices] for key, val in encodings.items()}\n",
    "\n",
    "train_main_labels = [main_labels[i] for i in train_indices]\n",
    "val_main_labels = [main_labels[i] for i in val_indices]\n",
    "\n",
    "train_fine_labels = [fine_labels[i] for i in train_indices]\n",
    "val_fine_labels = [fine_labels[i] for i in val_indices]\n",
    "\n",
    "train_dataset = EntityFramingDataset(train_encodings, train_main_labels, train_fine_labels)\n",
    "eval_dataset = EntityFramingDataset(val_encodings, val_main_labels, val_fine_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,  # Add the evaluation dataset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Trainer: evaluation requires an eval_dataset.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(results)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\trainer.py:3861\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3858\u001b[0m \u001b[38;5;66;03m# memory metrics - must set up as early as possible\u001b[39;00m\n\u001b[0;32m   3859\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_memory_tracker\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m-> 3861\u001b[0m eval_dataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_eval_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fsdp_xla_v2_enabled:\n\u001b[0;32m   3863\u001b[0m     eval_dataloader \u001b[38;5;241m=\u001b[39m tpu_spmd_dataloader(eval_dataloader)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\trainer.py:965\u001b[0m, in \u001b[0;36mTrainer.get_eval_dataloader\u001b[1;34m(self, eval_dataset)\u001b[0m\n\u001b[0;32m    955\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;124;03mReturns the evaluation [`~torch.utils.data.DataLoader`].\u001b[39;00m\n\u001b[0;32m    957\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    962\u001b[0m \u001b[38;5;124;03m        If a `str`, will use `self.eval_dataset[eval_dataset]` as the evaluation dataset. If a `Dataset`, will override `self.eval_dataset` and must implement `__len__`. If it is a [`~datasets.Dataset`], columns not accepted by the `model.forward()` method are automatically removed.\u001b[39;00m\n\u001b[0;32m    963\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eval_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 965\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrainer: evaluation requires an eval_dataset.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    967\u001b[0m \u001b[38;5;66;03m# If we have persistent workers, don't do a fork bomb especially as eval datasets\u001b[39;00m\n\u001b[0;32m    968\u001b[0m \u001b[38;5;66;03m# don't change during training\u001b[39;00m\n\u001b[0;32m    969\u001b[0m dataloader_key \u001b[38;5;241m=\u001b[39m eval_dataset \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(eval_dataset, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: Trainer: evaluation requires an eval_dataset."
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "print(results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
